{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f676de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878f9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5485602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    movies_data = {'Name': [], 'Rating': [], 'Year of Release': []}\n",
    "\n",
    "    movie_containers = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "    for movie in movie_containers:\n",
    "        try:\n",
    "            name = movie.h3.a.text.strip()\n",
    "        except AttributeError:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            rating = float(movie.find('span', class_='ipl-rating-star__rating').text)\n",
    "        except (AttributeError, ValueError):\n",
    "            rating = None\n",
    "\n",
    "        try:\n",
    "            year = int(movie.find('span', class_='lister-item-year').text.strip('()'))\n",
    "        except (AttributeError, ValueError):\n",
    "            year = None\n",
    "\n",
    "        movies_data['Name'].append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b540464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_meesho_bags(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting product details\n",
    "    product_details = []\n",
    "    for product in soup.find_all('div', class_='product-card'):\n",
    "        name = product.find('h3', class_='product-title').text\n",
    "        price = float(product.find('span', class_='product-price').text)\n",
    "        discount = float(product.find('span', class_='product-discount').text.strip('%'))\n",
    "        product_details.append({'Name': name, 'Price': price, 'Discount': discount})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame(product_details)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "url_meesho_bags = 'https://www.meesho.com/bags-ladies/pl/3jo?page=1'\n",
    "df_meesho_bags = scrape_meesho_bags(url_meesho_bags)\n",
    "print(df_meesho_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2c65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd51abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_icc_cricket_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # a) Top 10 ODI teams in men’s cricket\n",
    "    banners = soup.find_all('tr', class_='rankings-block__banner')\n",
    "    table_rows = soup.find_all('tr', class_='table-body')\n",
    "\n",
    "    teams_details = [\n",
    "        {\n",
    "            'Team': team.find('span', class_='u-hide-phablet').text,\n",
    "            'Matches': int(matches.text),\n",
    "            'Points': int(points.text),\n",
    "            'Rating': int(rating.text)\n",
    "        }\n",
    "        for team, matches, points, rating in zip(banners, table_rows, table_rows, table_rows)\n",
    "    ]\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_teams = pd.DataFrame(teams_details)\n",
    "    return df_teams\n",
    "\n",
    "# Example usage:\n",
    "url_icc_cricket_rankings = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "df_teams = scrape_icc_cricket_rankings(url_icc_cricket_rankings)\n",
    "print(df_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e91962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_patreon_posts(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting post details\n",
    "    post_details = []\n",
    "    for post in soup.find_all('div', class_='post-content'):\n",
    "        heading = post.find('h3', class_='post-card__headline').text\n",
    "        date = post.find('time', class_='post-card__date')['datetime']\n",
    "        content = post.find('div', class_='post-card__content').text.strip()\n",
    "        likes = int(post.find('span', class_='post-card__like-count').text)\n",
    "        post_details.append({'Heading': heading, 'Date': date, 'Content': content, 'Likes': likes})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_posts = pd.DataFrame(post_details)\n",
    "    return df_posts\n",
    "\n",
    "# Example usage:\n",
    "url_patreon_posts = 'https://www.patreon.com/coreyms'\n",
    "df_patreon_posts = scrape_patreon_posts(url_patreon_posts)\n",
    "print(df_patreon_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b867b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_nobroker_houses(localities):\n",
    "    house_details = []\n",
    "\n",
    "    for locality in localities:\n",
    "        url = f'https://www.nobroker.in/property/sale/{locality}?searchParam=W3sibGF0IjoxMi45NDAyMTQ1LCJsb24iOjc3LjgzODU5NzYyNTk1MTY2fV0=&radius=2.0'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extracting house details\n",
    "        for house in soup.find_all('div', class_='card'):\n",
    "            title = house.find('h2', class_='heading-6').text.strip()\n",
    "            location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "            area = house.find('div', class_='nb__3oNyC').text.strip().split(' ')[0]\n",
    "            emi = house.find('div', class_='font-semi-bold').text.strip().split(' ')[1]\n",
    "            price = float(house.find('div', class_='heading-7').text.replace('₹', '').replace('L', ''))\n",
    "\n",
    "            house_details.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_houses = pd.DataFrame(house_details)\n",
    "    return df_houses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb76df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb0fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae03ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name Price Image_URL\n",
      "0  N/A  None       N/A\n",
      "1  N/A  None       N/A\n",
      "2  N/A  None       N/A\n",
      "3  N/A  None       N/A\n",
      "4  N/A  None       N/A\n",
      "5  N/A  None       N/A\n",
      "6  N/A  None       N/A\n",
      "7  N/A  None       N/A\n",
      "8  N/A  None       N/A\n",
      "9  N/A  None       N/A\n"
     ]
    }
   ],
   "source": [
    "# question 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_bewakoof_products(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract product details\n",
    "    product_details = []\n",
    "    for product in soup.select('.productCardBox'):\n",
    "        name_elem = product.find('h3', class_='product-title')\n",
    "        price_elem = product.find('span', class_='original-price')\n",
    "        img_elem = product.find('div', class_='product-image')\n",
    "\n",
    "        # Check if elements are found before extract\n",
    "        if name_elem:\n",
    "            name = name_elem.text.strip()\n",
    "        else:\n",
    "            name = 'N/A'\n",
    "\n",
    "        if price_elem:\n",
    "            price = float(price_elem.text.replace('₹', '').replace(',', ''))\n",
    "        else:\n",
    "            price = None\n",
    "\n",
    "        if img_elem and img_elem.img:\n",
    "            img_url = img_elem.img.get('src', 'N/A')\n",
    "        else:\n",
    "            img_url = 'N/A'\n",
    "\n",
    "        product_details.append({'Name': name, 'Price': price, 'Image_URL': img_url})\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df_products = pd.DataFrame(product_details)\n",
    "    return df_products\n",
    "\n",
    "# Example usage:\n",
    "url_bewakoof_products = 'https://www.bewakoof.com/bestseller?sort=popula'\n",
    "df_bewakoof_products = scrape_bewakoof_products(url_bewakoof_products)\n",
    "print(df_bewakoof_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b42634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
