{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLtGdJ9y847f0j2mx8z6S9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moni8210/Assignment-3/blob/main/shakspeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgjhWVrAEGSK",
        "outputId": "87d7d8b5-18af-4fc5-c847-279c68b40df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Activation,LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "# corrected URL by removing the double slash after googleapis.com\n",
        "file=tf.keras.utils.get_file(\"shakespeare.txt\",\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text=open(file,\"rb\").read().decode(encoding=\"utf-8\")\n",
        "# in the above statement we open a file in binary mode(rb)then we decode thet mode into character\n",
        "# print(text[:1000]) it show the first thousands characters\n",
        "text=text[300000:800000]\n",
        "# it show the character between 3lc to 8lc\n",
        "characters=sorted(set(text))\n",
        "#print(text[:1000])\n",
        "#print(characters)\n",
        "char_to_index=dict((c,i) for i,c in enumerate(characters))\n",
        "# above statement convert the character into index it also create a like a box with seperate stickers\n",
        "index_to_char=dict((i,c) for i,c in enumerate(characters))\n",
        "seq_length=40\n",
        "stepsize=3\n",
        "#above statement take 40 characters at once then it read by skipping 3 charaters then read\n",
        "sentences=[]\n",
        "next_chars=[]\n",
        "for i in range(0,len(text)-seq_length,stepsize):\n",
        "  sentences.append(text[i:i+seq_length])\n",
        "  next_chars.append(text[i+seq_length])\n",
        "x=np.zeros((len(sentences),seq_length,len(characters)),dtype=bool)\n",
        "y=np.zeros((len(sentences),len(characters)),dtype=bool)\n",
        "for i,satz in enumerate(sentences):\n",
        "  for t,char in enumerate(satz):\n",
        "    x[i,t,char_to_index[char]]=1\n",
        "  y[i,char_to_index[next_chars[i]]]=1\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(LSTM(128,input_shape=(seq_length,len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(lr=0.01))\n",
        "model.fit(x,y,batch_size=256,epochs=4)\n",
        "  #in the above statement we create a models in  this firstly we give amemory to LSTM(long and short term memory)with 128neurons\n",
        "  #then we dense(ikathaa krna) the hidden layers which can increase the complecity of program then we use softmax activation function in order to make a result add upto one which gives probability for each character\n",
        "  # then select batchsize 256 in 4 epochs which the result again and again 4 times\n",
        "\n",
        "def sample(preds,temperature=1.0):\n",
        "  preds =np.asarray(preds).astype(\"float64\")\n",
        "  preds=np.log(preds)/temperature\n",
        "  exp_preds=np.exp(preds)\n",
        "  preds=exp_preds/np.sum(exp_preds)\n",
        "  probas=np.random.multinomial(1,preds,1)\n",
        "  return np.argmax(probas)\n",
        "def generate(length,temperature):\n",
        "  start_index=random.randint(0,len(text)-seq_length-1)\n",
        "  generated=\"\"\n",
        "  sentence=text[start_index:start_index+seq_length]\n",
        "  generated+=sentence\n",
        "  for i in range(length):\n",
        "    x_pred=np.zeros((1,seq_length,len(characters)))\n",
        "    for t,char in enumerate(sentence):\n",
        "      x_pred[0,t,char_to_index[char]]=1\n",
        "    preds=model.predict(x_pred,verbose=0)[0]\n",
        "    next_index=sample(preds,temperature)\n",
        "    next_char=index_to_char[next_index]\n",
        "    generated+=next_char\n",
        "    sentence=sentence[1:]+next_char\n",
        "  return generated\n",
        "print(generate(1000,0.2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7syjoKKEO4I",
        "outputId": "27217ba8-3bc1-43b6-eac6-6caa7a2ad5b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "651/651 [==============================] - 125s 189ms/step - loss: 2.8753\n",
            "Epoch 2/4\n",
            "651/651 [==============================] - 123s 189ms/step - loss: 2.3535\n",
            "Epoch 3/4\n",
            "651/651 [==============================] - 121s 186ms/step - loss: 2.2142\n",
            "Epoch 4/4\n",
            "651/651 [==============================] - 121s 186ms/step - loss: 2.1279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(1000,0.6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9QFM5lV7NE",
        "outputId": "a240639f-af16-4f74-d196-7ba6031af485"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y hands:\n",
            "Would none but I might venge my fire.\n",
            "\n",
            "KAREN:\n",
            "And have to me leake, four? I the beat there ave cone for theresst\n",
            "And sou hise thar sour meplaty that hes upere\n",
            "\n",
            "O'A I HeRY YI:\n",
            "I ther I lake the thee I ha kisher hill bent\n",
            "for hat in an Whill my ham was the kis shat hesh me in than and\n",
            "And has the heen bester mare the with wimy,\n",
            "My lor pean thace tathe seade mo wis mest.\n",
            "\n",
            "SARIME:\n",
            "But the hour hat houth his he thee is and to shald Rincome.\n",
            "\n",
            "WARUCE:\n",
            "Se, I hon mach for sue wing se seant lo not me and hall I aroug,\n",
            "For will thy if an stear ig thet of I mend\n",
            "Fow lo dereach po best dith the beaty yourd het.\n",
            "\n",
            "OUAMES:\n",
            "Ot it and gath thee sand hish our thee the;\n",
            "I purin the prowend the the und somest be's asteredsis\n",
            "Go me soe the thith nome hear a hin thea wate mous and quath'd\n",
            "An you wour all my lare beat hour hare wand\n",
            "The wing this ancar'd speat me se ther wish ofer.\n",
            "\n",
            "WARRCKE:\n",
            "Soug, werish hithe shell dove the that to here\n",
            "Theer pall the ar thee the as choul theathing and,\n",
            "To sich and wear me the kerist the sare:\n",
            "No ceme hit\n"
          ]
        }
      ]
    }
  ]
}